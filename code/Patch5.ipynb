{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Patch5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1mUDx4uFpbS6jrD7lN-P7F7saefP_QJ31?usp=sharing)\n"
      ],
      "metadata": {
        "id": "LUHWmiJKJeT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required packages\n",
        "!python -m spacy download fr_core_news_sm\n",
        "#import fr_core_news_sm\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np "
      ],
      "metadata": {
        "id": "hZzfj6wDJesE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import additional packages\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import string\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS\n",
        "from spacy.lang.fr.examples import sentences \n",
        "from spacy.lang.fr import French\n"
      ],
      "metadata": {
        "id": "XKCGAMGcJhlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"https://raw.githubusercontent.com/Lirette2/DMML2021_Apple/main/data/training_data.csv\"\n",
        "\n",
        "df = pd.read_csv(path, index_col=0)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "y5GQiuMnJioL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n"
      ],
      "metadata": {
        "id": "85zu_2MKJkG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Base rate: the data-set is a bit balanced!\n",
        "df.difficulty.value_counts()"
      ],
      "metadata": {
        "id": "EPEMcppDJlgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "difficulty_count = df.groupby(\"difficulty\").count()\n",
        "plt.bar(difficulty_count.index.values, difficulty_count[\"sentence\"])\n",
        "plt.xlabel(\"Difficulty\")\n",
        "plt.ylabel(\"Number of Sentences\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ME21eOP3JnNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "round(df.difficulty.value_counts().max()/ len(df), 4)\n"
      ],
      "metadata": {
        "id": "SAvGGNXgJnu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tokening the date with spaCy\n"
      ],
      "metadata": {
        "id": "mgkMCvZjJpFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "punctuations"
      ],
      "metadata": {
        "id": "ili63GXFJrjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of stopwords\n",
        "#stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "stop_words = spacy.lang.fr.stop_words.STOP_WORDS\n",
        "\n",
        "list(stop_words)[:10]"
      ],
      "metadata": {
        "id": "xL0TgyiNJuBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load French language model\n",
        "import fr_core_news_sm\n",
        "#sp = spacy.load('en_core_web_sm')\n",
        "sp = fr_core_news_sm.load()\n",
        "\n",
        "# Create tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Create token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = sp(sentence)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
        "    ## alternative way\n",
        "    # mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # Return preprocessed list of tokens\n",
        "    return mytokens\n",
        "\n",
        "texts = df['sentence']\n",
        "\n",
        "# Tokenize texts\n",
        "processed_texts = []\n",
        "for text in texts:\n",
        "  processed_text = spacy_tokenizer(text)\n",
        "  processed_texts.append(processed_text)"
      ],
      "metadata": {
        "id": "ZXBHpt46Jua5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word embedding \n",
        "### Parameters: \n",
        "#     - min_count: minimum number of occurence of single word in corpus to be taken into account\n",
        "#     - size: dimension of the vectors representing the tokens\n",
        "#     - IMPORTANT: processed_texts must be a list of lists of tokens object!\n",
        "from gensim.models import Word2Vec\n",
        "word2vec = Word2Vec(processed_texts, min_count=2, size=100)\n",
        "vocab = word2vec.wv.vocab"
      ],
      "metadata": {
        "id": "RP3fNx62JwNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### **** ***************** **** ####\n",
        "#### **** RAW TEXT FEATURES **** ####\n",
        "#### **** ***************** **** ####\n",
        "\n",
        "# Count tokens per sentence\n",
        "def count_token(sent):\n",
        "  return(len(spacy_tokenizer(sent))) #spacy_tokenizer() to get tokens, len() to count them\n",
        "\n",
        "# Count raw words per sentence\n",
        "def count_words(sent):\n",
        "  return(len(sent.split())) #split() gives us individual words, len() counts them\n",
        "\n",
        "#Get all characters in a sentece\n",
        "def count_sentence_character(sent):\n",
        "  words = sent.split()\n",
        "  return(sum(len(word) for word in words))\n",
        "\n",
        "#Get average character length of word\n",
        "def count_avg_word_character(sent):\n",
        "  words = sent.split()\n",
        "  return(sum(len(word) for word in words) / len(words))\n",
        "\n",
        "def count_avg_token_character(sent):\n",
        "  words = spacy_tokenizer(sent)\n",
        "  if len(words) == 0:\n",
        "    return(0)\n",
        "  else:\n",
        "    return(sum(len(word) for word in words) / len(words))\n",
        "\n",
        "#Get min character length of word\n",
        "def count_min_word_character(sent):\n",
        "  words = sent.split()\n",
        "  return(min(len(word) for word in words))\n",
        "\n",
        "#Get max character length of word\n",
        "def count_max_word_character(sent):\n",
        "  words = sent.split()\n",
        "  return(max(len(word) for word in words))"
      ],
      "metadata": {
        "id": "uk5Dd_PmJ2F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### **** **************** **** ####\n",
        "#### **** LEXICAL FEATURES **** ####\n",
        "#### **** **************** **** ####\n",
        "\n",
        "# Lexical Diversity\n",
        "def lex_div_word(sent):\n",
        "  total_number_word = len(sent.split())\n",
        "  unique = set(sent.split())\n",
        "  return(len(unique)/total_number_word)\n",
        "#We don't apply for token, as the goal of the tokenzization is to be left with \n",
        "#unique tokens\n",
        "#For tokens, we should apply to the whole text as done by tfidf_vector\n",
        "\n",
        "# Lexical Density\n",
        "def lex_den_tokens(sent):\n",
        "  st = spacy_tokenizer(sent)\n",
        "  if len(st) == 0:\n",
        "    return(0)\n",
        "  else:\n",
        "    string = \" \".join([str(item) for item in st])\n",
        "    x = sp(string)\n",
        "    counter = 0 \n",
        "    for token in x:\n",
        "      if token.pos_ == \"NOUN\" or token.pos_ == \"ADJ\" or token.pos_ == \"VERB\" or token.pos_ == \"ADV\":\n",
        "        counter = counter + 1\n",
        "    return(counter/len(st))\n",
        "\n",
        "def lex_den_words(sent):\n",
        "  x = sp(sent)\n",
        "  counter = 0 \n",
        "  for token in x:\n",
        "    if token.pos_ == \"NOUN\" or token.pos_ == \"ADJ\" or token.pos_ == \"VERB\" or token.pos_ == \"ADV\":\n",
        "      counter = counter + 1\n",
        "  return(counter/len(x))\n",
        "\n",
        "# Words NOT in frequent list\n",
        "path = \"https://raw.githubusercontent.com/Lirette2/DMML2021_Apple/main/data/list_words.csv\"\n",
        "words = pd.read_csv(path, index_col=0)\n",
        "\n",
        "\n",
        "def words_list(sent):\n",
        "  unique = set(sent.split())\n",
        "  counter = 0\n",
        "  for word_in_sentence in unique:\n",
        "    for word_in_list in words.Mots:\n",
        "      if word_in_sentence == word_in_list:\n",
        "        counter = counter + 1\n",
        "        break#we stop comparing once the word in found, to make it faster\n",
        "  return(1-(counter/len(unique)))\n",
        "\n",
        "\n",
        "def token_list(sent):\n",
        "  unique = spacy_tokenizer(sent)\n",
        "  if len(unique) == 0:\n",
        "    return(0)\n",
        "  else:\n",
        "    counter = 0\n",
        "    for word_in_sentence in unique:\n",
        "      for word_in_list in words.Mots:\n",
        "        if word_in_sentence == word_in_list:\n",
        "          counter = counter + 1\n",
        "          break#we stop comparing once the word in found, to make it faster\n",
        "    return(1-(counter/len(unique)))\n",
        "\n",
        "#Word Embedding:\n",
        "def get_vector(sent):\n",
        "  token = spacy_tokenizer(sent)\n",
        "  for word in token: \n",
        "    if word in word2vec.wv.vocab:\n",
        "      return(np.mean(word2vec[word]))"
      ],
      "metadata": {
        "id": "WRJGAvtxJ4yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(data_to_process):\n",
        "  raw_word_count = pd.Series(data_to_process.sentence.apply(count_words),name=\"raw_word_count\")\n",
        "  token_count = pd.Series(data_to_process.sentence.apply(count_token),name=\"token_count\")\n",
        "  avg_chr_word = pd.Series(data_to_process.sentence.apply(count_avg_word_character),name=\"avg_chr_word\")\n",
        "  min_chr_word = pd.Series(data_to_process.sentence.apply(count_min_word_character),name=\"min_chr_word\")\n",
        "  max_chr_word = pd.Series(data_to_process.sentence.apply(count_max_word_character),name=\"max_chr_word\")\n",
        "  tot_chr_stn = pd.Series(data_to_process.sentence.apply(count_sentence_character),name=\"tot_chr_stn\") \n",
        "  avg_chr_token = pd.Series(data_to_process.sentence.apply(count_avg_token_character),name=\"avg_chr_token\")\n",
        "  diversity_word = pd.Series(data_to_process.sentence.apply(lex_div_word),name=\"diversity_word\")\n",
        "  density_word = pd.Series(data_to_process.sentence.apply(lex_den_words),name=\"density_word\")\n",
        "  density_token = pd.Series(data_to_process.sentence.apply(lex_den_tokens),name=\"density_token\")\n",
        "  freq_word_list = pd.Series(data_to_process.sentence.apply(words_list),name=\"freq_word_list\")\n",
        "  freq_token_list = pd.Series(data_to_process.sentence.apply(token_list),name=\"freq_token_list\")\n",
        "  token_wv = pd.Series(data_to_process.sentence.apply(get_vector),name=\"token_wv\")\n",
        "  \n",
        "  processed_df = pd.concat([data_to_process,raw_word_count,token_count,avg_chr_word,min_chr_word,max_chr_word,tot_chr_stn,avg_chr_token,\n",
        "                    diversity_word,density_word,density_token,freq_word_list,freq_token_list,token_wv],axis=1)\n",
        "  return(processed_df)\n",
        "\n",
        "def scale_data(df_to_scale):\n",
        "  scaler = MinMaxScaler()\n",
        "  col_to_scale = [\"raw_word_count\",\"token_count\",\"avg_chr_word\",\"min_chr_word\",\"max_chr_word\",\"tot_chr_stn\",\"avg_chr_token\",]\n",
        "  #no need for the others because they already are on a scale from 0 to 1\n",
        "  df_to_scale[col_to_scale]= scaler.fit_transform(df_to_scale[col_to_scale])\n",
        "  return(df_to_scale) \n"
      ],
      "metadata": {
        "id": "TFGRZdtQJ7aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = get_features(df)\n"
      ],
      "metadata": {
        "id": "0tKq0G-YJ9Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = scale_data(new_df)\n",
        "new_df[\"token_wv\"] = new_df[\"token_wv\"].fillna(0)\n",
        "new_df.token_wv.isna().sum()"
      ],
      "metadata": {
        "id": "Vd3eiO9IJ_3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features\n",
        "X = new_df[[\"sentence\",\"raw_word_count\",\"token_count\",\"avg_chr_word\",\"min_chr_word\",\"max_chr_word\",\"tot_chr_stn\",\"avg_chr_token\",\n",
        "                    \"diversity_word\",\"density_word\",\"density_token\",\"freq_word_list\",\n",
        "            \"freq_token_list\",\"token_wv\",]]# the features we want to analyze\n",
        "\n",
        "ylabels = new_df['difficulty'] # the labels, or answers, we want to test against\n",
        "\n",
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=1234, stratify=ylabels)\n",
        "\n",
        "X_train"
      ],
      "metadata": {
        "id": "y_nSFD6NKCEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train\n"
      ],
      "metadata": {
        "id": "QKux78yTKFqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "def evaluate(test, pred):\n",
        "  precision = precision_score(test, pred,average=None)\n",
        "  recall = recall_score(test, pred, average=None)\n",
        "  f1= f1_score(test, pred, average=None)\n",
        "  print(f'CONFUSION MATRIX:\\n{confusion_matrix(test, pred)}')\n",
        "  print(f\"ACCURACY SCORE:\\n{accuracy_score(test, pred) :.4f}\")\n",
        "  print(f'CLASSIFICATION REPORT:')\n",
        "  print(\"Precision:\\t {0:4f}\".format(precision_score(test, pred,average=\"macro\"))) \n",
        "  print(\"Recall:\\t {0:4f}\".format(recall_score(test, pred, average=\"macro\")))\n",
        "  print(\"F1_Score:\\t {0:4f}\".format(f1_score(test, pred, average=\"macro\")))"
      ],
      "metadata": {
        "id": "artKw9TAKGSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Define classifier\n",
        "classifier = LogisticRegression(multi_class=\"multinomial\",max_iter=1000)\n",
        "\n",
        "#Vectorizer\n",
        "tfidf_vector = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
        "\n",
        "#Column Transformer (to apply vectorizer to the right column)\n",
        "column_transformer = ColumnTransformer(\n",
        "    [(\"tfidf\", tfidf_vector, \"sentence\")],\n",
        "    remainder=\"passthrough\")\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([(\"tfidf\",column_transformer),(\"classifier\", classifier)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train_full2, y_train)\n",
        "# Predictions\n",
        "y_pred = pipe.predict(X_test_full2)\n",
        "\n",
        "# Evaluation - test set\n",
        "evaluate(y_test, y_pred)"
      ],
      "metadata": {
        "id": "II1YzDxRKI1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "#Create a Gaussian Classifier\n",
        "random_for=RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "pipe_rf = Pipeline([(\"tfidf\",column_transformer),('feature_selection',SelectFromModel(LinearSVC(penalty=\"l2\"))),(\"model\", random_for)])\n",
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "pipe_rf.fit(X_train_full2,y_train)\n",
        "\n",
        "y_pred=pipe_rf.predict(X_test_full2)\n",
        "evaluate(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "F3h-VR94KMuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "\n",
        "#Train the model\n",
        "lsvc = LinearSVC(verbose=0)\n",
        "\n",
        "pipe_lsvc = Pipeline([(\"tfidf\",column_transformer),(\"model\", lsvc)])\n",
        "pipe_lsvc.fit(X_train_full2,y_train)\n",
        "y_pred=pipe_lsvc.predict(X_test_full2)\n",
        "evaluate(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "LRkPxnqhKOc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Submission data\n",
        "\n",
        "path = \"https://raw.githubusercontent.com/Lirette2/DMML2021_Apple/main/data/unlabelled_test_data.csv\"\n",
        "sub_df = pd.read_csv(path, index_col=0)\n",
        "\n",
        "new_sub_df = get_features(sub_df)"
      ],
      "metadata": {
        "id": "h7OKHEIIKPDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sub_df = scale_data(new_sub_df)\n",
        "new_sub_df[\"token_wv\"] = new_sub_df[\"token_wv\"].fillna(0)\n",
        "#new_sub_df.token_wv.isna().sum()"
      ],
      "metadata": {
        "id": "l2xK6H0eKUaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_sub = new_sub_df[[\"sentence\",\"raw_word_count\",\"token_count\",\"avg_chr_word\",\"min_chr_word\",\"max_chr_word\",\"tot_chr_stn\",\"avg_chr_token\",\n",
        "                    \"diversity_word\",\"density_word\",\"density_token\",\"freq_word_list\",\n",
        "            \"freq_token_list\",\"token_wv\"]]"
      ],
      "metadata": {
        "id": "Ls9S6wTxKV1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_sub = pipe.predict(X_sub)\n"
      ],
      "metadata": {
        "id": "8kgDfFUIKZNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sub_df[\"difficulty\"] = y_pred_sub\n",
        "submission = new_sub_df.filter([\"id\",\"difficulty\"],axis=1)\n",
        "submission"
      ],
      "metadata": {
        "id": "-qIkrsldKbfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "submission.to_csv('submission_18_apple_unil.csv') \n",
        "files.download('submission_18_apple_unil.csv')"
      ],
      "metadata": {
        "id": "DxvlpCc1KcxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Doc2Vec \n",
        "\n",
        "Here, I have some troubles, mostly because I tried to include the existing regressors, to the new model with Doc2Vec. I will serapate only put without the other regressors, and the submission"
      ],
      "metadata": {
        "id": "2d-Vma4hKdXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Doc2Vec\n",
        "# Not sure how correct this precedure is\n",
        "#I'm following: https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4\n",
        "\n",
        "#Multiprocessing, this allows code to run faster because it uses all the CPU available\n",
        "import multiprocessing\n",
        "cores = multiprocessing.cpu_count()\n",
        "\n",
        "# Getting the texts with the correct difficulty tags\n",
        "texts_tagged = df.apply(\n",
        "    lambda r: TaggedDocument(words=spacy_tokenizer(r.sentence), tags=[r.difficulty]), axis=1)"
      ],
      "metadata": {
        "id": "3bQFL8SWKptO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Doc2Vec: Part 2\n",
        "#We now build the the vocabulary\n",
        "\n",
        "model_dbow = Doc2Vec(dm=0, vector_size=35, negative=6, hs=0, min_count=1, sample = 0, workers=cores)\n",
        "model_dbow.build_vocab([x for x in texts_tagged.values])"
      ],
      "metadata": {
        "id": "uVK6nqxyKsws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dbow.train(texts_tagged, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs)\n"
      ],
      "metadata": {
        "id": "vzq5asrsKuDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Doc2Vec: Part 3\n",
        "\n",
        "def vec_for_learning(model, tagged_docs):\n",
        "    sents = tagged_docs.values\n",
        "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
        "    return targets, regressors"
      ],
      "metadata": {
        "id": "Bk7yJUF8KvXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tagged, test_tagged = train_test_split(texts_tagged, test_size=0.2, random_state=1234)\n",
        "\n",
        "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
        "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
      ],
      "metadata": {
        "id": "D9FS_qs2Kw0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "#Train the model\n",
        "lsvc = LinearSVC(verbose=0)\n",
        "\n",
        "# Fit model on training set\n",
        "lsvc.fit(X_train, y_train)\n",
        "# Predictions\n",
        "y_pred = lsvc.predict(X_test)\n",
        "\n",
        "# Evaluation - test set\n",
        "evaluate(y_test, y_pred)\n",
        "#0.6615\n",
        "#0.6594 # vector_size = 35,neg=6,min_count=2"
      ],
      "metadata": {
        "id": "Ryf-E5l1KyLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Define classifier\n",
        "classifier = LogisticRegression(multi_class=\"multinomial\",max_iter=1000, solver=\"lbfgs\")\n",
        "\n",
        "# Fit model on training set\n",
        "classifier.fit(X_train, y_train)\n",
        "# Predictions\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluation - test set\n",
        "evaluate(y_test, y_pred)"
      ],
      "metadata": {
        "id": "JooF_huzK0ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CNMoBLbSK178"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}